{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 神经网络模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk_c2_s0.1_b16_lr0.001_d0.5_e20\n",
      "models/model_sk_c2_s0.1_b16_lr0.001_d0.5_e20.pth\n",
      "True\n",
      "1\n",
      "GeForce MX150\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Para:\n",
    "    # tensor_board_log_dir = 'runs/exp0'\n",
    "    feature_column_start_name = 'ep_ratio_ttm'\n",
    "    feature_column_end_name = 'BR'\n",
    "\n",
    "    # 模型设置\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    classification = 2 # 2, 3\n",
    "\n",
    "    # 权重\n",
    "    cross_weight = list()\n",
    "    if classification == 3:\n",
    "        cross_weight = [1.0, 1.0 ,1.0]\n",
    "    elif classification == 2:\n",
    "        cross_weight = [1.0, 1.0]\n",
    "    elif classification == 5:\n",
    "        cross_weight = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "    batch_size = 16\n",
    "    lr = 1e-3\n",
    "    drop = 0.5\n",
    "    epochs = 20\n",
    "\n",
    "    # 数据集设置\n",
    "    month_in_sample = range(0, 1)\n",
    "    # month_test = range(36, 48)\n",
    "\n",
    "    percent_cv = 0.1 # 10% cross validation\n",
    "\n",
    "    data_path = 'data/sk_space_1d_rate_20d_12-21_pre'\n",
    "\n",
    "\n",
    "    seed = 2022\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    info_str0 = data_path[5:7]+'_'+'c'+str(classification)+'_s'+str(percent_cv)\n",
    "    info_str1 = '_b'+str(batch_size)+'_lr'+str(lr)+'_d'+str(drop)+'_e'+str(epochs)\n",
    "    info_str = info_str0 + info_str1\n",
    "\n",
    "    save_model_path = 'models/'+'model_'+info_str+'.pth'\n",
    "\n",
    "para = Para()\n",
    "print(para.info_str)\n",
    "print(para.save_model_path)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      return_bin order_book_id board_type sector_code  month        date  \\\n0            1.0   600376.XSHG  MainBoard  RealEstate      0  2012-01-04   \n1            0.0   600376.XSHG  MainBoard  RealEstate      1  2012-01-05   \n2            0.0   600376.XSHG  MainBoard  RealEstate      2  2012-01-06   \n3            0.0   600376.XSHG  MainBoard  RealEstate      3  2012-01-09   \n4            1.0   600376.XSHG  MainBoard  RealEstate      4  2012-01-10   \n...          ...           ...        ...         ...    ...         ...   \n2384         0.0   600376.XSHG  MainBoard  RealEstate   2406  2021-11-29   \n2385         0.0   600376.XSHG  MainBoard  RealEstate   2407  2021-11-30   \n2386         0.0   600376.XSHG  MainBoard  RealEstate   2408  2021-12-01   \n2387         0.0   600376.XSHG  MainBoard  RealEstate   2409  2021-12-02   \n2388         0.0   600376.XSHG  MainBoard  RealEstate   2410  2021-12-03   \n\n      yield_rate  ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  ...     RSI10  \\\n0      -0.141230     -1.524457      0.643879     -1.212339  ...  0.538128   \n1       0.253379     -1.500471      0.570059     -1.201655  ...  0.330607   \n2       0.749352     -1.503539      0.579287     -1.203022  ...  0.650744   \n3       0.065030     -1.536937      0.683865     -1.217897  ...  0.743422   \n4      -0.213278     -1.564457      0.776140     -1.230155  ...  0.650744   \n...          ...           ...           ...           ...  ...       ...   \n2384    1.462886      1.657715     -1.359218      3.640367  ... -0.065573   \n2385    1.647394      1.683664     -1.364536      3.640367  ... -0.143578   \n2386    1.530037      1.632091     -1.353900      3.640367  ...  0.447523   \n2387    1.484967      1.565309     -1.339718      3.599144  ...  0.917404   \n2388    1.535847      1.598423     -1.346809      3.640367  ...  0.386110   \n\n            SY    BIAS20     VOL30     VOL60    VOL120    VOLT20    VOLT60  \\\n0     1.224282  0.934854  0.186499  0.250212  0.183803 -0.329742 -0.644819   \n1     1.224282  0.417093  0.217907  0.263290  0.180967 -0.340798 -0.635916   \n2     1.224282  0.428205  0.211118  0.248994  0.178574 -0.338216 -0.627350   \n3     1.224282  0.985446  0.229623  0.245450  0.177982 -0.305186 -0.609930   \n4     1.224282  1.428671  0.267379  0.249163  0.186044 -0.233927 -0.574659   \n...        ...       ...       ...       ...       ...       ...       ...   \n2384 -0.882930  0.052101 -0.646078 -0.707748 -0.780226 -1.025460 -0.470301   \n2385 -0.882930 -0.060633 -0.653847 -0.708004 -0.780349 -1.031112 -0.460610   \n2386 -0.180526  0.148899 -0.655539 -0.708258 -0.779669 -1.028788 -0.454117   \n2387 -0.180526  0.403522 -0.652971 -0.707706 -0.778008 -0.992226 -0.452088   \n2388 -0.180526  0.232531 -0.672819 -0.707329 -0.776899 -0.996948 -0.451957   \n\n            AR        BR  \n0     2.494509  1.125687  \n1     2.293300  0.968585  \n2     1.857657  0.677275  \n3     1.762400  0.498572  \n4     2.099490  0.715619  \n...        ...       ...  \n2384 -1.132233 -1.416920  \n2385 -0.648826 -0.976654  \n2386 -0.258064 -0.658283  \n2387  0.532003 -0.119379  \n2388  0.610689 -0.188933  \n\n[2389 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>return_bin</th>\n      <th>order_book_id</th>\n      <th>board_type</th>\n      <th>sector_code</th>\n      <th>month</th>\n      <th>date</th>\n      <th>yield_rate</th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>...</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>600376.XSHG</td>\n      <td>MainBoard</td>\n      <td>RealEstate</td>\n      <td>0</td>\n      <td>2012-01-04</td>\n      <td>-0.141230</td>\n      <td>-1.524457</td>\n      <td>0.643879</td>\n      <td>-1.212339</td>\n      <td>...</td>\n      <td>0.538128</td>\n      <td>1.224282</td>\n      <td>0.934854</td>\n      <td>0.186499</td>\n      <td>0.250212</td>\n      <td>0.183803</td>\n      <td>-0.329742</td>\n      <td>-0.644819</td>\n      <td>2.494509</td>\n      <td>1.125687</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>600376.XSHG</td>\n      <td>MainBoard</td>\n      <td>RealEstate</td>\n      <td>1</td>\n      <td>2012-01-05</td>\n      <td>0.253379</td>\n      <td>-1.500471</td>\n      <td>0.570059</td>\n      <td>-1.201655</td>\n      <td>...</td>\n      <td>0.330607</td>\n      <td>1.224282</td>\n      <td>0.417093</td>\n      <td>0.217907</td>\n      <td>0.263290</td>\n      <td>0.180967</td>\n      <td>-0.340798</td>\n      <td>-0.635916</td>\n      <td>2.293300</td>\n      <td>0.968585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>600376.XSHG</td>\n      <td>MainBoard</td>\n      <td>RealEstate</td>\n      <td>2</td>\n      <td>2012-01-06</td>\n      <td>0.749352</td>\n      <td>-1.503539</td>\n      <td>0.579287</td>\n      <td>-1.203022</td>\n      <td>...</td>\n      <td>0.650744</td>\n      <td>1.224282</td>\n      <td>0.428205</td>\n      <td>0.211118</td>\n      <td>0.248994</td>\n      <td>0.178574</td>\n      <td>-0.338216</td>\n      <td>-0.627350</td>\n      <td>1.857657</td>\n      <td>0.677275</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>600376.XSHG</td>\n      <td>MainBoard</td>\n      <td>RealEstate</td>\n      <td>3</td>\n      <td>2012-01-09</td>\n      <td>0.065030</td>\n      <td>-1.536937</td>\n      <td>0.683865</td>\n      <td>-1.217897</td>\n      <td>...</td>\n      <td>0.743422</td>\n      <td>1.224282</td>\n      <td>0.985446</td>\n      <td>0.229623</td>\n      <td>0.245450</td>\n      <td>0.177982</td>\n      <td>-0.305186</td>\n      <td>-0.609930</td>\n      <td>1.762400</td>\n      <td>0.498572</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>600376.XSHG</td>\n      <td>MainBoard</td>\n      <td>RealEstate</td>\n      <td>4</td>\n      <td>2012-01-10</td>\n      <td>-0.213278</td>\n      <td>-1.564457</td>\n      <td>0.776140</td>\n      <td>-1.230155</td>\n      <td>...</td>\n      <td>0.650744</td>\n      <td>1.224282</td>\n      <td>1.428671</td>\n      <td>0.267379</td>\n      <td>0.249163</td>\n      <td>0.186044</td>\n      <td>-0.233927</td>\n      <td>-0.574659</td>\n      <td>2.099490</td>\n      <td>0.715619</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2384</th>\n      <td>0.0</td>\n      <td>600376.XSHG</td>\n      <td>MainBoard</td>\n      <td>RealEstate</td>\n      <td>2406</td>\n      <td>2021-11-29</td>\n      <td>1.462886</td>\n      <td>1.657715</td>\n      <td>-1.359218</td>\n      <td>3.640367</td>\n      <td>...</td>\n      <td>-0.065573</td>\n      <td>-0.882930</td>\n      <td>0.052101</td>\n      <td>-0.646078</td>\n      <td>-0.707748</td>\n      <td>-0.780226</td>\n      <td>-1.025460</td>\n      <td>-0.470301</td>\n      <td>-1.132233</td>\n      <td>-1.416920</td>\n    </tr>\n    <tr>\n      <th>2385</th>\n      <td>0.0</td>\n      <td>600376.XSHG</td>\n      <td>MainBoard</td>\n      <td>RealEstate</td>\n      <td>2407</td>\n      <td>2021-11-30</td>\n      <td>1.647394</td>\n      <td>1.683664</td>\n      <td>-1.364536</td>\n      <td>3.640367</td>\n      <td>...</td>\n      <td>-0.143578</td>\n      <td>-0.882930</td>\n      <td>-0.060633</td>\n      <td>-0.653847</td>\n      <td>-0.708004</td>\n      <td>-0.780349</td>\n      <td>-1.031112</td>\n      <td>-0.460610</td>\n      <td>-0.648826</td>\n      <td>-0.976654</td>\n    </tr>\n    <tr>\n      <th>2386</th>\n      <td>0.0</td>\n      <td>600376.XSHG</td>\n      <td>MainBoard</td>\n      <td>RealEstate</td>\n      <td>2408</td>\n      <td>2021-12-01</td>\n      <td>1.530037</td>\n      <td>1.632091</td>\n      <td>-1.353900</td>\n      <td>3.640367</td>\n      <td>...</td>\n      <td>0.447523</td>\n      <td>-0.180526</td>\n      <td>0.148899</td>\n      <td>-0.655539</td>\n      <td>-0.708258</td>\n      <td>-0.779669</td>\n      <td>-1.028788</td>\n      <td>-0.454117</td>\n      <td>-0.258064</td>\n      <td>-0.658283</td>\n    </tr>\n    <tr>\n      <th>2387</th>\n      <td>0.0</td>\n      <td>600376.XSHG</td>\n      <td>MainBoard</td>\n      <td>RealEstate</td>\n      <td>2409</td>\n      <td>2021-12-02</td>\n      <td>1.484967</td>\n      <td>1.565309</td>\n      <td>-1.339718</td>\n      <td>3.599144</td>\n      <td>...</td>\n      <td>0.917404</td>\n      <td>-0.180526</td>\n      <td>0.403522</td>\n      <td>-0.652971</td>\n      <td>-0.707706</td>\n      <td>-0.778008</td>\n      <td>-0.992226</td>\n      <td>-0.452088</td>\n      <td>0.532003</td>\n      <td>-0.119379</td>\n    </tr>\n    <tr>\n      <th>2388</th>\n      <td>0.0</td>\n      <td>600376.XSHG</td>\n      <td>MainBoard</td>\n      <td>RealEstate</td>\n      <td>2410</td>\n      <td>2021-12-03</td>\n      <td>1.535847</td>\n      <td>1.598423</td>\n      <td>-1.346809</td>\n      <td>3.640367</td>\n      <td>...</td>\n      <td>0.386110</td>\n      <td>-0.180526</td>\n      <td>0.232531</td>\n      <td>-0.672819</td>\n      <td>-0.707329</td>\n      <td>-0.776899</td>\n      <td>-0.996948</td>\n      <td>-0.451957</td>\n      <td>0.610689</td>\n      <td>-0.188933</td>\n    </tr>\n  </tbody>\n</table>\n<p>2389 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_in_sample = None\n",
    "for i_month in para.month_in_sample:\n",
    "    file_name = para.data_path + '/' + str(i_month) + '.csv'\n",
    "    data_curr_month = pd.read_csv(file_name)\n",
    "\n",
    "    data_curr_month = data_curr_month.dropna(axis=0)\n",
    "\n",
    "    data_curr_month.insert(loc=0, column='return_bin', value=np.nan)\n",
    "\n",
    "    data_curr_month.loc[data_curr_month['yield_rate']>0, 'return_bin'] = 0\n",
    "    data_curr_month.loc[data_curr_month['yield_rate']<=0, 'return_bin'] = 1\n",
    "\n",
    "    if i_month == para.month_in_sample[0]:\n",
    "        data_in_sample = data_curr_month\n",
    "    else:\n",
    "        data_in_sample = pd.concat([data_in_sample, data_curr_month])\n",
    "        # data_in_sample = data_in_sample.append(data_curr_month)\n",
    "\n",
    "data_in_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  MACD_DIFF  MACD_DEA  \\\n0        -1.524457      0.643879     -1.212339   0.965225  0.697316   \n1        -1.500471      0.570059     -1.201655   0.896273  0.745902   \n2        -1.503539      0.579287     -1.203022   0.840603  0.773069   \n3        -1.536937      0.683865     -1.217897   0.889795  0.805143   \n4        -1.564457      0.776140     -1.230155   1.008477  0.855748   \n...            ...           ...           ...        ...       ...   \n2145     -0.243384     -1.009849      1.050885  -0.230040 -0.284554   \n2146     -0.239937     -1.011728      1.055094  -0.231966 -0.276739   \n2147     -0.190519     -1.038037      1.115448  -0.273718 -0.279262   \n2148     -0.186903     -1.039916      1.119863  -0.306850 -0.288245   \n2149     -0.150094     -1.058708      1.164818  -0.360092 -0.306622   \n\n      MACD_HIST     RSI10        SY    BIAS20     VOL30     VOL60    VOL120  \\\n0      0.990854  0.538128  1.224282  0.934854  0.186499  0.250212  0.183803   \n1      0.605944  0.330607  1.224282  0.417093  0.217907  0.263290  0.180967   \n2      0.333522  0.650744  1.224282  0.428205  0.211118  0.248994  0.178574   \n3      0.395923  0.743422  1.224282  0.985446  0.229623  0.245450  0.177982   \n4      0.631619  0.650744  1.224282  1.428671  0.267379  0.249163  0.186044   \n...         ...       ...       ...       ...       ...       ...       ...   \n2145   0.118673 -0.350847 -0.882930 -0.151494 -0.772571 -0.707969 -0.698921   \n2146   0.087384 -0.350847 -1.585334 -0.178863 -0.772718 -0.708999 -0.699805   \n2147  -0.044115 -1.053790 -1.585334 -0.519861 -0.770928 -0.709570 -0.700236   \n2148  -0.126274 -0.928130 -1.585334 -0.524438 -0.771887 -0.712030 -0.701534   \n2149  -0.245758 -1.141207 -1.585334 -0.743372 -0.771685 -0.719058 -0.704034   \n\n        VOLT20    VOLT60        AR        BR  \n0    -0.329742 -0.644819  2.494509  1.125687  \n1    -0.340798 -0.635916  2.293300  0.968585  \n2    -0.338216 -0.627350  1.857657  0.677275  \n3    -0.305186 -0.609930  1.762400  0.498572  \n4    -0.233927 -0.574659  2.099490  0.715619  \n...        ...       ...       ...       ...  \n2145 -1.008562 -0.377083 -0.982731  0.303978  \n2146 -1.011489 -0.369160 -0.830266  0.417762  \n2147 -0.934769 -0.355120 -0.675094  0.659123  \n2148 -0.872603 -0.341957 -0.733134  0.823248  \n2149 -0.768779 -0.332726 -1.145100  0.188543  \n\n[2150 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>MACD_DIFF</th>\n      <th>MACD_DEA</th>\n      <th>MACD_HIST</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.524457</td>\n      <td>0.643879</td>\n      <td>-1.212339</td>\n      <td>0.965225</td>\n      <td>0.697316</td>\n      <td>0.990854</td>\n      <td>0.538128</td>\n      <td>1.224282</td>\n      <td>0.934854</td>\n      <td>0.186499</td>\n      <td>0.250212</td>\n      <td>0.183803</td>\n      <td>-0.329742</td>\n      <td>-0.644819</td>\n      <td>2.494509</td>\n      <td>1.125687</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.500471</td>\n      <td>0.570059</td>\n      <td>-1.201655</td>\n      <td>0.896273</td>\n      <td>0.745902</td>\n      <td>0.605944</td>\n      <td>0.330607</td>\n      <td>1.224282</td>\n      <td>0.417093</td>\n      <td>0.217907</td>\n      <td>0.263290</td>\n      <td>0.180967</td>\n      <td>-0.340798</td>\n      <td>-0.635916</td>\n      <td>2.293300</td>\n      <td>0.968585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.503539</td>\n      <td>0.579287</td>\n      <td>-1.203022</td>\n      <td>0.840603</td>\n      <td>0.773069</td>\n      <td>0.333522</td>\n      <td>0.650744</td>\n      <td>1.224282</td>\n      <td>0.428205</td>\n      <td>0.211118</td>\n      <td>0.248994</td>\n      <td>0.178574</td>\n      <td>-0.338216</td>\n      <td>-0.627350</td>\n      <td>1.857657</td>\n      <td>0.677275</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.536937</td>\n      <td>0.683865</td>\n      <td>-1.217897</td>\n      <td>0.889795</td>\n      <td>0.805143</td>\n      <td>0.395923</td>\n      <td>0.743422</td>\n      <td>1.224282</td>\n      <td>0.985446</td>\n      <td>0.229623</td>\n      <td>0.245450</td>\n      <td>0.177982</td>\n      <td>-0.305186</td>\n      <td>-0.609930</td>\n      <td>1.762400</td>\n      <td>0.498572</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.564457</td>\n      <td>0.776140</td>\n      <td>-1.230155</td>\n      <td>1.008477</td>\n      <td>0.855748</td>\n      <td>0.631619</td>\n      <td>0.650744</td>\n      <td>1.224282</td>\n      <td>1.428671</td>\n      <td>0.267379</td>\n      <td>0.249163</td>\n      <td>0.186044</td>\n      <td>-0.233927</td>\n      <td>-0.574659</td>\n      <td>2.099490</td>\n      <td>0.715619</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2145</th>\n      <td>-0.243384</td>\n      <td>-1.009849</td>\n      <td>1.050885</td>\n      <td>-0.230040</td>\n      <td>-0.284554</td>\n      <td>0.118673</td>\n      <td>-0.350847</td>\n      <td>-0.882930</td>\n      <td>-0.151494</td>\n      <td>-0.772571</td>\n      <td>-0.707969</td>\n      <td>-0.698921</td>\n      <td>-1.008562</td>\n      <td>-0.377083</td>\n      <td>-0.982731</td>\n      <td>0.303978</td>\n    </tr>\n    <tr>\n      <th>2146</th>\n      <td>-0.239937</td>\n      <td>-1.011728</td>\n      <td>1.055094</td>\n      <td>-0.231966</td>\n      <td>-0.276739</td>\n      <td>0.087384</td>\n      <td>-0.350847</td>\n      <td>-1.585334</td>\n      <td>-0.178863</td>\n      <td>-0.772718</td>\n      <td>-0.708999</td>\n      <td>-0.699805</td>\n      <td>-1.011489</td>\n      <td>-0.369160</td>\n      <td>-0.830266</td>\n      <td>0.417762</td>\n    </tr>\n    <tr>\n      <th>2147</th>\n      <td>-0.190519</td>\n      <td>-1.038037</td>\n      <td>1.115448</td>\n      <td>-0.273718</td>\n      <td>-0.279262</td>\n      <td>-0.044115</td>\n      <td>-1.053790</td>\n      <td>-1.585334</td>\n      <td>-0.519861</td>\n      <td>-0.770928</td>\n      <td>-0.709570</td>\n      <td>-0.700236</td>\n      <td>-0.934769</td>\n      <td>-0.355120</td>\n      <td>-0.675094</td>\n      <td>0.659123</td>\n    </tr>\n    <tr>\n      <th>2148</th>\n      <td>-0.186903</td>\n      <td>-1.039916</td>\n      <td>1.119863</td>\n      <td>-0.306850</td>\n      <td>-0.288245</td>\n      <td>-0.126274</td>\n      <td>-0.928130</td>\n      <td>-1.585334</td>\n      <td>-0.524438</td>\n      <td>-0.771887</td>\n      <td>-0.712030</td>\n      <td>-0.701534</td>\n      <td>-0.872603</td>\n      <td>-0.341957</td>\n      <td>-0.733134</td>\n      <td>0.823248</td>\n    </tr>\n    <tr>\n      <th>2149</th>\n      <td>-0.150094</td>\n      <td>-1.058708</td>\n      <td>1.164818</td>\n      <td>-0.360092</td>\n      <td>-0.306622</td>\n      <td>-0.245758</td>\n      <td>-1.141207</td>\n      <td>-1.585334</td>\n      <td>-0.743372</td>\n      <td>-0.771685</td>\n      <td>-0.719058</td>\n      <td>-0.704034</td>\n      <td>-0.768779</td>\n      <td>-0.332726</td>\n      <td>-1.145100</td>\n      <td>0.188543</td>\n    </tr>\n  </tbody>\n</table>\n<p>2150 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in_sample = data_in_sample.loc[:, para.feature_column_start_name: para.feature_column_end_name]\n",
    "y_in_sample = data_in_sample.loc[:, 'return_bin']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, shuffle=False) # True, random_state=para.seed)\n",
    "# X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, shuffle=False)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0       1.0\n1       0.0\n2       0.0\n3       0.0\n4       1.0\n       ... \n2145    1.0\n2146    1.0\n2147    1.0\n2148    1.0\n2149    1.0\nName: return_bin, Length: 2150, dtype: float64"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  MACD_DIFF  MACD_DEA  \\\n0        -1.524457      0.643879     -1.212339   0.965225  0.697316   \n1        -1.500471      0.570059     -1.201655   0.896273  0.745902   \n2        -1.503539      0.579287     -1.203022   0.840603  0.773069   \n3        -1.536937      0.683865     -1.217897   0.889795  0.805143   \n4        -1.564457      0.776140     -1.230155   1.008477  0.855748   \n...            ...           ...           ...        ...       ...   \n2145     -0.243384     -1.009849      1.050885  -0.230040 -0.284554   \n2146     -0.239937     -1.011728      1.055094  -0.231966 -0.276739   \n2147     -0.190519     -1.038037      1.115448  -0.273718 -0.279262   \n2148     -0.186903     -1.039916      1.119863  -0.306850 -0.288245   \n2149     -0.150094     -1.058708      1.164818  -0.360092 -0.306622   \n\n      MACD_HIST     RSI10        SY    BIAS20     VOL30     VOL60    VOL120  \\\n0      0.990854  0.538128  1.224282  0.934854  0.186499  0.250212  0.183803   \n1      0.605944  0.330607  1.224282  0.417093  0.217907  0.263290  0.180967   \n2      0.333522  0.650744  1.224282  0.428205  0.211118  0.248994  0.178574   \n3      0.395923  0.743422  1.224282  0.985446  0.229623  0.245450  0.177982   \n4      0.631619  0.650744  1.224282  1.428671  0.267379  0.249163  0.186044   \n...         ...       ...       ...       ...       ...       ...       ...   \n2145   0.118673 -0.350847 -0.882930 -0.151494 -0.772571 -0.707969 -0.698921   \n2146   0.087384 -0.350847 -1.585334 -0.178863 -0.772718 -0.708999 -0.699805   \n2147  -0.044115 -1.053790 -1.585334 -0.519861 -0.770928 -0.709570 -0.700236   \n2148  -0.126274 -0.928130 -1.585334 -0.524438 -0.771887 -0.712030 -0.701534   \n2149  -0.245758 -1.141207 -1.585334 -0.743372 -0.771685 -0.719058 -0.704034   \n\n        VOLT20    VOLT60        AR        BR  return_bin  \n0    -0.329742 -0.644819  2.494509  1.125687         1.0  \n1    -0.340798 -0.635916  2.293300  0.968585         0.0  \n2    -0.338216 -0.627350  1.857657  0.677275         0.0  \n3    -0.305186 -0.609930  1.762400  0.498572         0.0  \n4    -0.233927 -0.574659  2.099490  0.715619         1.0  \n...        ...       ...       ...       ...         ...  \n2145 -1.008562 -0.377083 -0.982731  0.303978         1.0  \n2146 -1.011489 -0.369160 -0.830266  0.417762         1.0  \n2147 -0.934769 -0.355120 -0.675094  0.659123         1.0  \n2148 -0.872603 -0.341957 -0.733134  0.823248         1.0  \n2149 -0.768779 -0.332726 -1.145100  0.188543         1.0  \n\n[2150 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>MACD_DIFF</th>\n      <th>MACD_DEA</th>\n      <th>MACD_HIST</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n      <th>return_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.524457</td>\n      <td>0.643879</td>\n      <td>-1.212339</td>\n      <td>0.965225</td>\n      <td>0.697316</td>\n      <td>0.990854</td>\n      <td>0.538128</td>\n      <td>1.224282</td>\n      <td>0.934854</td>\n      <td>0.186499</td>\n      <td>0.250212</td>\n      <td>0.183803</td>\n      <td>-0.329742</td>\n      <td>-0.644819</td>\n      <td>2.494509</td>\n      <td>1.125687</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.500471</td>\n      <td>0.570059</td>\n      <td>-1.201655</td>\n      <td>0.896273</td>\n      <td>0.745902</td>\n      <td>0.605944</td>\n      <td>0.330607</td>\n      <td>1.224282</td>\n      <td>0.417093</td>\n      <td>0.217907</td>\n      <td>0.263290</td>\n      <td>0.180967</td>\n      <td>-0.340798</td>\n      <td>-0.635916</td>\n      <td>2.293300</td>\n      <td>0.968585</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.503539</td>\n      <td>0.579287</td>\n      <td>-1.203022</td>\n      <td>0.840603</td>\n      <td>0.773069</td>\n      <td>0.333522</td>\n      <td>0.650744</td>\n      <td>1.224282</td>\n      <td>0.428205</td>\n      <td>0.211118</td>\n      <td>0.248994</td>\n      <td>0.178574</td>\n      <td>-0.338216</td>\n      <td>-0.627350</td>\n      <td>1.857657</td>\n      <td>0.677275</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.536937</td>\n      <td>0.683865</td>\n      <td>-1.217897</td>\n      <td>0.889795</td>\n      <td>0.805143</td>\n      <td>0.395923</td>\n      <td>0.743422</td>\n      <td>1.224282</td>\n      <td>0.985446</td>\n      <td>0.229623</td>\n      <td>0.245450</td>\n      <td>0.177982</td>\n      <td>-0.305186</td>\n      <td>-0.609930</td>\n      <td>1.762400</td>\n      <td>0.498572</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.564457</td>\n      <td>0.776140</td>\n      <td>-1.230155</td>\n      <td>1.008477</td>\n      <td>0.855748</td>\n      <td>0.631619</td>\n      <td>0.650744</td>\n      <td>1.224282</td>\n      <td>1.428671</td>\n      <td>0.267379</td>\n      <td>0.249163</td>\n      <td>0.186044</td>\n      <td>-0.233927</td>\n      <td>-0.574659</td>\n      <td>2.099490</td>\n      <td>0.715619</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2145</th>\n      <td>-0.243384</td>\n      <td>-1.009849</td>\n      <td>1.050885</td>\n      <td>-0.230040</td>\n      <td>-0.284554</td>\n      <td>0.118673</td>\n      <td>-0.350847</td>\n      <td>-0.882930</td>\n      <td>-0.151494</td>\n      <td>-0.772571</td>\n      <td>-0.707969</td>\n      <td>-0.698921</td>\n      <td>-1.008562</td>\n      <td>-0.377083</td>\n      <td>-0.982731</td>\n      <td>0.303978</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2146</th>\n      <td>-0.239937</td>\n      <td>-1.011728</td>\n      <td>1.055094</td>\n      <td>-0.231966</td>\n      <td>-0.276739</td>\n      <td>0.087384</td>\n      <td>-0.350847</td>\n      <td>-1.585334</td>\n      <td>-0.178863</td>\n      <td>-0.772718</td>\n      <td>-0.708999</td>\n      <td>-0.699805</td>\n      <td>-1.011489</td>\n      <td>-0.369160</td>\n      <td>-0.830266</td>\n      <td>0.417762</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2147</th>\n      <td>-0.190519</td>\n      <td>-1.038037</td>\n      <td>1.115448</td>\n      <td>-0.273718</td>\n      <td>-0.279262</td>\n      <td>-0.044115</td>\n      <td>-1.053790</td>\n      <td>-1.585334</td>\n      <td>-0.519861</td>\n      <td>-0.770928</td>\n      <td>-0.709570</td>\n      <td>-0.700236</td>\n      <td>-0.934769</td>\n      <td>-0.355120</td>\n      <td>-0.675094</td>\n      <td>0.659123</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2148</th>\n      <td>-0.186903</td>\n      <td>-1.039916</td>\n      <td>1.119863</td>\n      <td>-0.306850</td>\n      <td>-0.288245</td>\n      <td>-0.126274</td>\n      <td>-0.928130</td>\n      <td>-1.585334</td>\n      <td>-0.524438</td>\n      <td>-0.771887</td>\n      <td>-0.712030</td>\n      <td>-0.701534</td>\n      <td>-0.872603</td>\n      <td>-0.341957</td>\n      <td>-0.733134</td>\n      <td>0.823248</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2149</th>\n      <td>-0.150094</td>\n      <td>-1.058708</td>\n      <td>1.164818</td>\n      <td>-0.360092</td>\n      <td>-0.306622</td>\n      <td>-0.245758</td>\n      <td>-1.141207</td>\n      <td>-1.585334</td>\n      <td>-0.743372</td>\n      <td>-0.771685</td>\n      <td>-0.719058</td>\n      <td>-0.704034</td>\n      <td>-0.768779</td>\n      <td>-0.332726</td>\n      <td>-1.145100</td>\n      <td>0.188543</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2150 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.concat([X_train, y_train], axis=1)\n",
    "data_cv = pd.concat([X_cv, y_cv], axis=1)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "\n",
    "X_train_ndarray = X_train.values\n",
    "y_train_ndarray = y_train.values\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_ndarray).type(torch.FloatTensor), torch.from_numpy(y_train_ndarray).type(torch.LongTensor))\n",
    "\n",
    "# for X_train_temp, y_train_temp in train_dataset:\n",
    "#     print(X_train_temp, y_train_temp)\n",
    "#     print(X_train_temp.dtype, y_train_temp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_cv_ndarray = X_cv.values\n",
    "y_cv_ndarray = y_cv.values\n",
    "\n",
    "cv_dataset = TensorDataset(torch.from_numpy(X_cv_ndarray).type(torch.FloatTensor), torch.from_numpy(y_cv.values).type(torch.LongTensor))\n",
    "\n",
    "# for X_cv_temp, y_cv_temp in cv_dataset:\n",
    "#     print(X_cv_temp, y_cv_temp)\n",
    "#     print(X_cv_temp.dtype, y_cv_temp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=para.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cv_dataloader = DataLoader(\n",
    "    dataset=cv_dataset,\n",
    "    batch_size=para.batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data_test = None\n",
    "# for i_month in para.month_test:\n",
    "#\n",
    "#     file_name = para.data_path + '/' + str(i_month) + '.csv'\n",
    "#     data_curr_month = pd.read_csv(file_name)\n",
    "#\n",
    "#     data_curr_month = data_curr_month.dropna(axis=0)\n",
    "#\n",
    "#     data_curr_month = label_data(data=data_curr_month, percent_select=para.percent_select)\n",
    "#\n",
    "#     if i_month == para.month_test[0]:\n",
    "#         data_test = data_curr_month\n",
    "#     else:\n",
    "#         data_test = pd.concat([data_test, data_curr_month])\n",
    "#         # data_test = data_test.append(data_curr_month)\n",
    "#\n",
    "# X_test = data_test.loc[:, para.feature_column_start_name: para.feature_column_end_name]\n",
    "# y_test = data_test.loc[:, 'return_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset\n",
    "# import torch\n",
    "#\n",
    "#\n",
    "# X_test_ndarray = X_test.values\n",
    "# y_test_ndarray = y_test.values\n",
    "#\n",
    "# test_dataset = TensorDataset(torch.from_numpy(X_test_ndarray).type(torch.FloatTensor), torch.from_numpy(y_test_ndarray).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "#\n",
    "#\n",
    "# test_dataloader = DataLoader(\n",
    "#     dataset=test_dataset,\n",
    "#     batch_size=para.batch_size,\n",
    "#     shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from my_utils.model_class import MLP\n",
    "\n",
    "model = MLP(in_nums=len(X_train.columns), out_nums=para.classification, drop_p=para.drop)\n",
    "# to device\n",
    "model = model.to(device=para.device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    # initialize metric\n",
    "    train_precision = torchmetrics.Precision(average='none', num_classes=para.classification).to(device=para.device)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # to device\n",
    "        X = X.to(device=para.device)\n",
    "        y = y.to(device=para.device)\n",
    "\n",
    "        # compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        train_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # metric on current batch\n",
    "        train_precision(pred.argmax(1), y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if batch % 10 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Train Error: \\n    Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "\n",
    "    # metric on all batches using custom accumulation\n",
    "    total_precision = train_precision.compute()\n",
    "    print(\"Precision of every train dataset class: \", total_precision)\n",
    "    print()\n",
    "\n",
    "    return correct, train_loss, total_precision\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # initialize metric\n",
    "    test_precision = torchmetrics.Precision(average='none', num_classes=para.classification).to(device=para.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            # to device\n",
    "            X = X.to(device=para.device)\n",
    "            y = y.to(device=para.device)\n",
    "\n",
    "            # compute prediction and loss\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            # metric on current batch\n",
    "            test_precision(pred.argmax(1), y)\n",
    "\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n    Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    # metric on all batches using custom accumulation\n",
    "    total_precision = test_precision.compute()\n",
    "    print(\"Precision of every test dataset class: \", total_precision)\n",
    "    print()\n",
    "\n",
    "    return correct, test_loss, total_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_df_to_dataloader(df: pd.DataFrame, select: int) -> DataLoader:\n",
    "\n",
    "    df = df[df['return_bin'] == select]\n",
    "\n",
    "    df_dataset = TensorDataset(\n",
    "        torch.from_numpy(df.loc[:, para.feature_column_start_name: para.feature_column_end_name].values).type(torch.FloatTensor),\n",
    "        torch.from_numpy(df.loc[:, 'return_bin'].values).type(torch.LongTensor))\n",
    "\n",
    "    df_dataloader = DataLoader(\n",
    "        dataset=df_dataset,\n",
    "        batch_size=para.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return df_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temp2_dataloader = select_df_to_dataloader(df=data_cv, select=2)\n",
    "temp1_dataloader = select_df_to_dataloader(df=data_cv, select=1)\n",
    "temp0_dataloader = select_df_to_dataloader(df=data_cv, select=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 51.0%, Avg loss: 0.693752 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.4395, 0.5368], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.0%, Avg loss: 0.679700 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7037, 0.6321], device='cuda:0')\n",
      "\n",
      "Time cost = 1.106068s\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 53.4%, Avg loss: 0.690061 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.4510, 0.5427], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 62.8%, Avg loss: 0.677834 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6147], device='cuda:0')\n",
      "\n",
      "Time cost = 1.982250s\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 53.7%, Avg loss: 0.688645 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.4454, 0.5426], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 59.4%, Avg loss: 0.677572 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5941], device='cuda:0')\n",
      "\n",
      "Time cost = 2.838149s\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 53.3%, Avg loss: 0.686941 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.4393, 0.5417], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 60.3%, Avg loss: 0.674991 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.5992], device='cuda:0')\n",
      "\n",
      "Time cost = 3.710952s\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 54.6%, Avg loss: 0.681909 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5170, 0.5486], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 65.3%, Avg loss: 0.678800 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7059, 0.6439], device='cuda:0')\n",
      "\n",
      "Time cost = 4.558077s\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 54.6%, Avg loss: 0.682048 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5107, 0.5513], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.9%, Avg loss: 0.679925 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7358, 0.6882], device='cuda:0')\n",
      "\n",
      "Time cost = 5.415150s\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.9%, Avg loss: 0.678741 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5862, 0.5658], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.4%, Avg loss: 0.675447 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7021, 0.6667], device='cuda:0')\n",
      "\n",
      "Time cost = 6.260962s\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.4%, Avg loss: 0.679313 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5728, 0.5627], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.2%, Avg loss: 0.680760 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6780, 0.6833], device='cuda:0')\n",
      "\n",
      "Time cost = 7.217396s\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.6%, Avg loss: 0.675021 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5566, 0.5678], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.6%, Avg loss: 0.675217 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7292, 0.6754], device='cuda:0')\n",
      "\n",
      "Time cost = 8.203320s\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.7%, Avg loss: 0.675687 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5613, 0.5677], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.4%, Avg loss: 0.679800 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6610, 0.6778], device='cuda:0')\n",
      "\n",
      "Time cost = 9.179466s\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.3%, Avg loss: 0.670990 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5689, 0.5735], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.0%, Avg loss: 0.673189 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7255, 0.6809], device='cuda:0')\n",
      "\n",
      "Time cost = 10.125600s\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.3%, Avg loss: 0.668265 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5654, 0.5759], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.5%, Avg loss: 0.670152 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7400, 0.6825], device='cuda:0')\n",
      "\n",
      "Time cost = 11.066818s\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.4%, Avg loss: 0.674484 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5642, 0.5770], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.2%, Avg loss: 0.671893 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7561, 0.6667], device='cuda:0')\n",
      "\n",
      "Time cost = 12.014169s\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.9%, Avg loss: 0.670971 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5782, 0.5787], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 66.5%, Avg loss: 0.671343 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7931, 0.6476], device='cuda:0')\n",
      "\n",
      "Time cost = 13.001115s\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.0%, Avg loss: 0.670199 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5936, 0.5885], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.6%, Avg loss: 0.673132 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7500, 0.6718], device='cuda:0')\n",
      "\n",
      "Time cost = 13.952752s\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.6%, Avg loss: 0.665774 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6075, 0.5926], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.8%, Avg loss: 0.672085 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7941, 0.6585], device='cuda:0')\n",
      "\n",
      "Time cost = 14.902265s\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.2%, Avg loss: 0.665814 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6252, 0.5953], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.2%, Avg loss: 0.669067 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7692, 0.6650], device='cuda:0')\n",
      "\n",
      "Time cost = 15.876868s\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 58.2%, Avg loss: 0.660770 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5727, 0.5852], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.4%, Avg loss: 0.673923 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7714, 0.6569], device='cuda:0')\n",
      "\n",
      "Time cost = 16.831420s\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.5%, Avg loss: 0.656764 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5900, 0.5975], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.4%, Avg loss: 0.671722 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7209, 0.6633], device='cuda:0')\n",
      "\n",
      "Time cost = 17.821661s\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.5%, Avg loss: 0.666318 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5612, 0.5797], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 66.1%, Avg loss: 0.672776 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7000, 0.6533], device='cuda:0')\n",
      "\n",
      "Time cost = 18.817014s\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 计时\n",
    "time_start = time.time()\n",
    "\n",
    "# writer = SummaryWriter(para.tensor_board_log_dir)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# to device\n",
    "loss_fn = loss_fn.to(device=para.device)\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=para.lr)\n",
    "\n",
    "epochs = para.epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    model.train()\n",
    "    accuracy_train, loss_train, precision_train = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    model.eval()\n",
    "    accuracy_cv, loss_cv, precision_cv = test_loop(cv_dataloader, model, loss_fn)\n",
    "\n",
    "    # accuracy2 = test_loop(temp2_dataloader, model, loss_fn)\n",
    "    # print('#')\n",
    "    # accuracy1 = test_loop(temp1_dataloader, model, loss_fn)\n",
    "    # accuracy0 = test_loop(temp0_dataloader, model, loss_fn)\n",
    "\n",
    "    # 写入 tensorboard\n",
    "    if para.classification == 2:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1]},\n",
    "                           global_step=t)\n",
    "\n",
    "    elif para.classification == 3:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1],\n",
    "                               'precision2': precision_cv[2]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1],\n",
    "                               'precision2': precision_train[2]},\n",
    "                           global_step=t)\n",
    "\n",
    "    elif para.classification == 5:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1],\n",
    "                               'precision2': precision_cv[2],\n",
    "                               'precision3': precision_cv[3],\n",
    "                               'precision4': precision_cv[4]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1],\n",
    "                               'precision2': precision_train[2],\n",
    "                               'precision3': precision_train[3],\n",
    "                               'precision4': precision_train[4]},\n",
    "                           global_step=t)\n",
    "\n",
    "    writer.add_scalars(main_tag=para.info_str+'_loss/cv',\n",
    "                       tag_scalar_dict={\n",
    "                           'loss': loss_cv},\n",
    "                       global_step=t)\n",
    "\n",
    "    writer.add_scalars(main_tag=para.info_str+'_loss/train',\n",
    "                       tag_scalar_dict={\n",
    "                           'loss': loss_train},\n",
    "                       global_step=t)\n",
    "    writer.flush()\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('Time cost = %fs' % (time_end - time_start))\n",
    "    print()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 保存模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish save model!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), para.save_model_path)\n",
    "\n",
    "print('Finish save model!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # captum\n",
    "# from captum.attr import IntegratedGradients\n",
    "#\n",
    "# ig = IntegratedGradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temp = cv_dataloader.dataset.tensors[0]\n",
    "# temp.requires_grad_()\n",
    "# attr, delta = ig.attribute(temp,target=1, return_convergence_delta=True)\n",
    "# attr = attr.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Helper method to print importances and visualize distribution\n",
    "# def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "#     print(title)\n",
    "#     for i in range(len(feature_names)):\n",
    "#         print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "#     y_pos = (np.arange(len(feature_names)))\n",
    "#     if plot:\n",
    "#         plt.figure(figsize=(20,6))\n",
    "#         plt.barh(y_pos, importances, align='center')\n",
    "#         plt.yticks(y_pos, feature_names)\n",
    "#         plt.ylabel(axis_title)\n",
    "#         plt.grid(axis='y')\n",
    "#         plt.title(title)\n",
    "# visualize_importances(feature_names=X_cv.columns.values.tolist(), importances=np.mean(attr, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_cv.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.Tensor(\n",
    "#     [[-0.0441,  0.0773],\n",
    "#     [-0.0781, -0.1772],\n",
    "#     [-0.1319, -0.0432],\n",
    "#     [-0.0714, -0.1261],\n",
    "#     [-0.0806, -0.1370],\n",
    "#     [-0.1730, -0.1472],\n",
    "#     [-0.0350, -0.0507],\n",
    "#     [-0.1149, -0.2248]])\n",
    "# # input = input.reshape(-1,4)\n",
    "# target = torch.Tensor([0, 1, 1, 0, 0, 0, 0, 0]).type(torch.LongTensor)\n",
    "# print(input.dtype)\n",
    "# print(target.dtype)\n",
    "# output = loss(input, target)\n",
    "# print(input, target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Example of target with class indices\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.Tensor([1,4,1]).type(torch.LongTensor)\n",
    "# output = loss(input, target)\n",
    "# print(input,target,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# input = torch.Tensor([0.5, 0.4, 0.3])\n",
    "# target = torch.Tensor([0])\n",
    "# output = loss(input, target)\n",
    "# print(input, target, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}